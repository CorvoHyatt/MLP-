{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2.2  Perceptron Multicapa\n",
    "## By Raul Ruvalcaba Barajas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener y Preparar los conjuntos de datos de MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\t\n",
    "import struct\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from NeuralNetMLP import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP:\n",
    "    def __init__(self, eta=0.01, epochs=50, random_state=None, init_weight=True, weights=None,\n",
    "                 shuffle=True, f_activate='sigmoid', n_hidden=[5], minibatch_size=1, loss_function='mse'):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.init_weight = init_weight\n",
    "        self.weights = weights\n",
    "        self.shuffle = shuffle\n",
    "        self.f_activate = f_activate\n",
    "        self.n_hidden = n_hidden\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.loss_function = loss_function\n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        w = np.array([[0.15, 0.25],[0.20, 0.30]])\n",
    "        #self.weights_.append(w)\n",
    "        w = np.array([[0.40, 0.50],[0.45, 0.55]])\n",
    "        #self.weights_.append(w)\n",
    "        b = np.array([[0.35, 0.35]])\n",
    "        #self.biases_.append(b)\n",
    "        b = np.array([0.60, 0.60])\n",
    "        #self.biases_.append(b)\n",
    "        \n",
    "\n",
    "    def _initialize_weights(self, X_train, y_train):\n",
    "        if self.init_weight:\n",
    "            if self.weights is not None:\n",
    "                self.weights_ = self.weights\n",
    "        else:\n",
    "            rgen = np.random.RandomState(self.random_state)\n",
    "            print(\"X_shape: \",X_train.shape[1]) \n",
    "            self.n_features_ = X_train.shape[1]\n",
    "            if len(y_train.shape) == 1:\n",
    "                self.n_output_ = 10\n",
    "            else:\n",
    "                self.n_output_ = y_train.shape[1]\n",
    "            \n",
    "            # Initialize weights\n",
    "            \n",
    "            weight = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                if i == 0:\n",
    "                    # Input layer to the first hidden layer\n",
    "\n",
    "                    for neurons in range(self.n_hidden[0]):\n",
    "                        weight = rgen.normal(loc=0.0, scale=0.01, size=(1, self.n_features_)) #TODO change\n",
    "                        self.weights_.append(weight)\n",
    "                else:\n",
    "                    # Hidden layers\n",
    "                    for neurons in range(self.n_hidden[i]):\n",
    "                        weight = rgen.normal(loc=0.0, scale=0.01, size=(1, self.n_features_))\n",
    "                        self.weights_.append(weight)\n",
    "                \n",
    "            \n",
    "            # Initialize biases\n",
    "            self.biases_ = [rgen.normal(loc=0.0, scale=0.01, size=(n, 1)) for n in self.n_hidden]\n",
    "            print(\"Weights: \",self.weights_)\n",
    "            print(\"Biases: \",self.biases_)\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def _tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def _forward(self, X):\n",
    "        # Lista para almacenar las activaciones en cada capa\n",
    "        activations = [] \n",
    "        # Capa de entrada\n",
    "        activation = X\n",
    "        activations.append(activation)\n",
    "        # Capas ocultas\n",
    "        for i in range(len(self.n_hidden)):\n",
    "            print(\"activations dim: \",activations[0].shape)\n",
    "            print(\"weights dim: \",self.weights_[0].shape)\n",
    "            print(\"a.w : dim\", np.dot(activations[0],self.weights_[0].T).shape)\n",
    "            print(\"biases dim: \",self.biases_[0].shape)\n",
    "            net_input = np.dot(activations[i], self.weights_[i].T) + self.biases_[i]\n",
    "            activation = self._sigmoid(net_input) if self.f_activate == 'sigmoid' else self._tanh(net_input)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Capa de salida\n",
    "        net_input = np.dot(activations[-1], self.weights_[-1].T) + self.biases_[-1]\n",
    "        activation = self._sigmoid(net_input) if self.f_activate == 'sigmoid' else self._tanh(net_input)\n",
    "        activations.append(activation)\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def _compute_loss(self, y_enc, output):\n",
    "        if self.loss_function == 'bce':\n",
    "            term1 = -y_enc * (np.log(output))\n",
    "            term2 = (1 - y_enc) * np.log(1 - output)\n",
    "            loss = np.sum(term1 - term2)\n",
    "        else:\n",
    "            loss = 0.5 * np.sum((y_enc - output) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def _get_gradient(self, activations, y_enc):\n",
    "        gradients = []\n",
    "\n",
    "        output_error = activations[-1] - y_enc\n",
    "\n",
    "        # Calcular la derivada de la función de activación en la capa de salida\n",
    "        if self.f_activate == 'sigmoid':\n",
    "            activation_derivative = activations[-1] * (1 - activations[-1])\n",
    "        else:\n",
    "            activation_derivative = 1.0 - activations[-1]**2\n",
    "\n",
    "        # Calcular el delta en la capa de salida y el gradiente de los pesos de salida\n",
    "        delta_output = output_error * activation_derivative\n",
    "        gradient_output = np.dot(activations[-2].T, delta_output)\n",
    "        gradients.append(gradient_output)\n",
    "\n",
    "        # Retropropagación a través de las capas ocultas\n",
    "        for i in range(len(self.weights_) - 2, -1, -1):\n",
    "            # Calcular la derivada de la función de activación en la capa oculta\n",
    "            if self.f_activate == 'sigmoid':\n",
    "                activation_derivative = activations[i + 1][:, 1:] * (1 - activations[i + 1][:, 1:])\n",
    "            else:\n",
    "                activation_derivative = 1.0 - activations[i + 1][:, 1:]**2\n",
    "\n",
    "            # Calcular el delta en la capa oculta y el gradiente de los pesos de la capa oculta\n",
    "            delta_hidden = np.dot(delta_output, self.weights_[i + 1][:, 1:]) * activation_derivative\n",
    "            gradient_hidden = np.dot(activations[i].T, delta_hidden)\n",
    "            gradients.insert(0, gradient_hidden)\n",
    "\n",
    "            # Actualizar el delta para la próxima iteración\n",
    "            delta_output = delta_hidden\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.n_features_ = X_train.shape[1]\n",
    "        if len(y_train.shape) == 1: #Si no esta definido y_train.shape[1]\n",
    "            self.n_output_ = 10\n",
    "        else:\n",
    "            self.n_output_ = y_train.shape[1]\n",
    "        self.cost_ = []\n",
    "\n",
    "        X_data, y_data = X_train.copy(), y_train.copy()\n",
    "        if not self.init_weight:\n",
    "            self._initialize_weights(X_train,y_train)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_data = X_data[idx], y_data[idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatch_size)\n",
    "            for idx in mini:\n",
    "                # feedforward\n",
    "                activations = self._forward(X_data[idx])\n",
    "\n",
    "                # compute loss\n",
    "                y_enc = y_data[idx]\n",
    "                output = activations[-1]\n",
    "                cost = self._compute_loss(y_enc, output)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradients via backpropagation\n",
    "                gradients = self._get_gradient(activations, y_enc)\n",
    "\n",
    "                # update weights\n",
    "                for i in range(len(self.weights_)):\n",
    "                    self.weights_[i] -= self.eta * gradients[i]\n",
    "\n",
    "            # compute validation loss\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_activations = self._forward(X_val)\n",
    "                val_output = val_activations[-1]\n",
    "                val_loss = self._compute_loss(y_val, val_output)\n",
    "                print(f'Epoch {epoch + 1}/{self.epochs}, Training Loss: {cost:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n",
      "Rows: 10000, columns: 784\n",
      "(10000,)\n",
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_val, y_val = load_mnist('', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_val.shape[0], X_val.shape[1]))\n",
    "print(y_val.shape)\n",
    "y_train = []\n",
    "for y in y_val:\n",
    "  aux = [0,0,0,0,0,0,0,0,0,0]\n",
    "  aux[y]=1\n",
    "  y_train.append(aux)\n",
    "y_val = np.insert(y_val,)\n",
    "y_train=np.array(y_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetMLP(eta=0.01, epochs=50, random_state=42, init_weight=False, n_hidden=[2],minibatch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0],[1, 0],[0, 1],[1, 1]])\n",
    "y = np.array([[0, 1],[1, 0],[1, 0],[0, 1]])\n",
    "nn.fit(X_train,y_train,X_val,y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
